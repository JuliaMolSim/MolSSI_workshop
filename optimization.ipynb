{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using StaticArrays\n",
    "using Random\n",
    "using Plots\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is just in time (JIT) compiled, so the first time you run code there will be a penalty to first compile the code before it can be run. We can use the `@time` macro to see this effect. (If you run the cell more than once, the effect will go away since it has already been compiled!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(1000);\n",
    "function sum_global()\n",
    "    s = 0.0\n",
    "    for i in x\n",
    "        s += i\n",
    "    end\n",
    "    return s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.011895 seconds (3.68 k allocations: 78.109 KiB, 98.53% compilation time)\n",
      "  0.000158 seconds (3.49 k allocations: 70.156 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time sum_global();\n",
    "@time sum_global();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@time` macro is convenient, but only executes the function once. We can use the [BenchmarkTools](https://juliaci.github.io/BenchmarkTools.jl/stable/) library to investigate the performance of our function with multiple trials. The `@benchmark` macro will run 10,000 trials or (roughly) 5 seconds whichever comes first. (These can be configured). There is also the `@btime` macro which runs the same number of trials as `@benchmark` but has output similar to `@time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m 73.742 μs\u001b[22m\u001b[39m … \u001b[35m202.248 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 99.94%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m 77.693 μs               \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m108.668 μs\u001b[22m\u001b[39m ± \u001b[32m  2.022 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m19.89% ±  2.97%\n",
       "\n",
       "  \u001b[39m▄\u001b[39m▅\u001b[39m█\u001b[34m▅\u001b[39m\u001b[39m▄\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▂\u001b[39m▁\u001b[32m▁\u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▇\u001b[39m▆\u001b[39m▇\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▆\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▅\u001b[39m▃\u001b[39m▄\u001b[39m▄\u001b[39m▃\u001b[39m▅\u001b[39m▄\u001b[39m▄\u001b[39m \u001b[39m█\n",
       "  73.7 μs\u001b[90m       \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m        164 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m70.16 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3490\u001b[39m."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark sum_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function takes parameters, be sure to \"interpolate\" them with a `$` when using `@benchmark` this tells BenchmarkTools to ignore the allocation and time required to allocate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.733 μs (4 allocations: 3.59 KiB)\n",
      "  1.951 μs (5 allocations: 3.95 KiB)\n"
     ]
    }
   ],
   "source": [
    "# Parameter is Interpolated\n",
    "@btime inv($(rand(6,6)));\n",
    "# No Interpolation\n",
    "@btime inv(rand(6,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Memory Allocations\n",
    "One of the easiest ways to speed up your code is by removing all unnecessary memory allocations. In Python the end-user is not able to manage memory easily and the interprer or library (e.g. numpy) is trusted to handle things.\n",
    "\n",
    "Julia can also operate like Python, where we just trust the compiler to \"do the right thing\". This is good for prototyping, but can result in inefficient code. Below we will look at the basics of memory allocations and how to reduce them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "Lets start by allocating a small array of 10,000 `Float64`s. Each `Float64` is 8 bytes (64 bits), so the total size should be 80,000 bytes or 80kB (78.125 kiB). Note there will be a slight overhead for some internal machinery associated with each allocation (e.g. a pointer to the array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000010 seconds (2 allocations: 78.172 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time ones(Float64, 100, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia all arrays are `heap` allocated since they are re-sizeable. This just means that the array is further away from the CPU and it can be slow(er) to allocate and access. The first strategy to mitigate this penalty is to simply allocate memory as little as possible. To check this we can use BenchmarkTools to understand the allocations in our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allocates_once (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function allocates_alot()\n",
    "    res = 0.0\n",
    "    for i in 1:100\n",
    "        x = rand(100,100) # ALLOCATION EVERY LOOP ITERATION\n",
    "        res += sum(x)\n",
    "    end\n",
    "    return res\n",
    "end\n",
    "\n",
    "function allocates_once()\n",
    "    res = 0.0\n",
    "    storage = zeros(100,100)\n",
    "    for i in 1:100\n",
    "        # The ! point means it mutates the input\n",
    "        # In this case it over writes storage with new random values\n",
    "        rand!(storage)\n",
    "        res += sum(storage)\n",
    "    end\n",
    "    return res\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `@btime` macro, we can measure the performace uplift and the number of allocations from each function. You'll find that the function which allocates a new array each loop iteration will be slower and allocate far more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.800 ms (200 allocations: 7.63 MiB)\n",
      "  1.052 ms (2 allocations: 78.17 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime allocates_alot() samples = 1000;\n",
    "@btime allocates_once() samples = 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more subtle, yet very common place where allocations occur is array slices (e.g. `arr[1:10]`). In Julia, these slices allocate a new array by default. We can get around this by telling Julia that we only want to read the data inside the arr. To do this we use an \"array view\" which can be acheived with the `view(...)` function or the `@views` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "view_macro_slices (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function slices_allocate(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    res = sum(x[1:N])\n",
    "    return res\n",
    "end\n",
    "\n",
    "function view_slices(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    #The view function takes the array\n",
    "    # and the indicies you want a view of.\n",
    "    res = sum(view(x, 1:N))\n",
    "    return res\n",
    "end\n",
    "\n",
    "function view_macro_slices(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    # The @views macro tells Julia all array slices in\n",
    "    # this line should be array views.\n",
    "    @views res = sum(x[1:N])\n",
    "    return res\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see a large speed increase and the number of allocations drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  35.536 μs (3 allocations: 390.69 KiB)\n",
      "  9.059 μs (1 allocation: 16 bytes)\n",
      "  9.053 μs (1 allocation: 16 bytes)\n"
     ]
    }
   ],
   "source": [
    "x = rand(100000)\n",
    "@btime slices_allocate(x) samples = 1000;\n",
    "@btime view_slices(x) samples = 1000;\n",
    "@btime view_macro_slices(x) samples = 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StaticArrays\n",
    "Earlier, we mentioned that Julia arrays are `heap` allocated because they have variable length (i.e., you can append to them). If you know the length of your array beforehand, you can use the [StaticArrays](https://juliaarrays.github.io/StaticArrays.jl/stable/) library to `stack` allocate your data (if its small enough).\n",
    "\n",
    "StaticArrays has many types you can use, but the main types are `SVector` for 1D arrays, `SMatrix` for 2D arrays, and `SArray` for n-dimensional arrays (all types have equivalent macros e.g., `@SMatrix`).\n",
    "\n",
    "To initialize a NxN `SMatrix` we must provide the size to the constructor: `SMatrix{N,N}(...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SMatrix{25,25}(rand(25,25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common pattern in atomistic simulation packages is to have an array of `SVectors` which represent atomic properties, like their position. Storing the data for each atom as an SVector can be much faster than using the default Julia array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distance_matrix (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_atoms = 1000\n",
    "static_positions = [SVector{3}(rand(3)) for i in 1:N_atoms];\n",
    "dynamic_positions = [rand(3) for i in 1:N_atoms];\n",
    "\n",
    "function distance_matrix(positions)\n",
    "    N = length(positions)\n",
    "    dists = zeros(N,N)\n",
    "    for i in 1:N\n",
    "        for j in i:N\n",
    "            dists[i,j] = norm(positions[i] - positions[j])\n",
    "            dists[j,i] = dists[i,j]\n",
    "        end\n",
    "    end\n",
    "    return dists\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static Vector Benchmark:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 1023 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m3.331 ms\u001b[22m\u001b[39m … \u001b[35m20.662 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m 0.00% … 60.81%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m4.364 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m 0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m4.862 ms\u001b[22m\u001b[39m ± \u001b[32m 1.739 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m13.46% ± 18.04%\n",
       "\n",
       "  \u001b[39m█\u001b[39m▆\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m▅\u001b[39m▃\u001b[39m▂\u001b[32m▁\u001b[39m\u001b[39m▁\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▄\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[32m█\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▇\u001b[39m▅\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▆\u001b[39m▅\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▅\u001b[39m \u001b[39m█\n",
       "  3.33 ms\u001b[90m      \u001b[39m\u001b[90mHistogram: \u001b[39m\u001b[90m\u001b[1mlog(\u001b[22m\u001b[39m\u001b[90mfrequency\u001b[39m\u001b[90m\u001b[1m)\u001b[22m\u001b[39m\u001b[90m by time\u001b[39m     10.1 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m7.63 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m2\u001b[39m."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Static Vector Benchmark:\")\n",
    "@benchmark distance_matrix(static_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Array Benchmark:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 104 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m39.539 ms\u001b[22m\u001b[39m … \u001b[35m86.751 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m5.22% … 2.37%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m49.074 ms              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m4.78%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m48.075 ms\u001b[22m\u001b[39m ± \u001b[32m 6.049 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m5.04% ± 1.62%\n",
       "\n",
       "  \u001b[39m▁\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m▃\u001b[32m \u001b[39m\u001b[39m▁\u001b[34m \u001b[39m\u001b[39m▁\u001b[39m█\u001b[39m \u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▁\u001b[39m \u001b[39m▃\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m█\u001b[39m▆\u001b[39m█\u001b[39m▆\u001b[39m▆\u001b[39m▄\u001b[39m█\u001b[39m▇\u001b[39m▇\u001b[39m█\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▇\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▄\u001b[39m▆\u001b[39m▆\u001b[39m█\u001b[32m▁\u001b[39m\u001b[39m█\u001b[34m▇\u001b[39m\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▇\u001b[39m█\u001b[39m▁\u001b[39m▇\u001b[39m▄\u001b[39m▆\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m▄\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▄\u001b[39m \u001b[39m▄\n",
       "  39.5 ms\u001b[90m         Histogram: frequency by time\u001b[39m        61.8 ms \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m45.81 MiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m500502\u001b[39m."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Dynamic Array Benchmark:\")\n",
    "@benchmark distance_matrix(dynamic_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `SVector`s etc. are statically sized and immutable. The StaticArrays package also provides statically sized but mutable variants, which are prefixed with `M` instead of `S`, e.g. `SMatrix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Stability\n",
    "Julia will do its best to infer the data types of your variables, but if the compiler cannot infer the type of all your variables this leads to type instability and will slow down your code. We can check type stability with the `@code_warntype` macro. For today's purposes, if this macro returns any red text, then your code has unstable types.\n",
    "\n",
    "In the example below, the functions are equivalent except one relies on a gloabl variable and the other takes that variable as a parameter. Because of the global variable, the compiler must assume that the variables inside the *unstable* function have `Any` type. Whereas in the type-stable code, the compiler can create a specialized version of the stable function for the specific type you passed into it. This is not the only unstable pattern, so if you really care about speed, check your code with `@code_warntype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stable (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_x = rand(Float64, 1000)\n",
    "function unstable()\n",
    "    s = 0\n",
    "    for val in global_x\n",
    "      s = s + val\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "function stable(x)\n",
    "  s = zero(eltype(x))\n",
    "  for val in x\n",
    "     s = s + val\n",
    "  end\n",
    "  return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MethodInstance for unstable()\n",
      "  from unstable()\u001b[90m @\u001b[39m \u001b[90mMain\u001b[39m \u001b[90mc:\\Users\\ejmei\\repos\\MolSSI_workshop\\\u001b[39m\u001b[90m\u001b[4mjl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X34sZmlsZQ==.jl:2\u001b[24m\u001b[39m\n",
      "Arguments\n",
      "  #self#\u001b[36m::Core.Const(unstable)\u001b[39m\n",
      "Locals\n",
      "  @_2\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "  s\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "  val\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "Body\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m       (s = 0)\n",
      "\u001b[90m│  \u001b[39m %2  = Main.global_x\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (@_2 = Base.iterate(%2))\n",
      "\u001b[90m│  \u001b[39m %4  = (@_2 === nothing)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m│  \u001b[39m %5  = Base.not_int(%4)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m└──\u001b[39m       goto #4 if not %5\n",
      "\u001b[90m2 ┄\u001b[39m %7  = @_2\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (val = Core.getfield(%7, 1))\n",
      "\u001b[90m│  \u001b[39m %9  = Core.getfield(%7, 2)\u001b[91m\u001b[1m::Any\u001b[22m\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (s = s + val)\n",
      "\u001b[90m│  \u001b[39m       (@_2 = Base.iterate(%2, %9))\n",
      "\u001b[90m│  \u001b[39m %12 = (@_2 === nothing)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m│  \u001b[39m %13 = Base.not_int(%12)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m└──\u001b[39m       goto #4 if not %13\n",
      "\u001b[90m3 ─\u001b[39m       goto #2\n",
      "\u001b[90m4 ┄\u001b[39m       return s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@code_warntype unstable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MethodInstance for stable(::Vector{Float64})\n",
      "  from stable(\u001b[90mx\u001b[39m)\u001b[90m @\u001b[39m \u001b[90mMain\u001b[39m \u001b[90m\u001b[4mIn[15]:10\u001b[24m\u001b[39m\n",
      "Arguments\n",
      "  #self#\u001b[36m::Core.Const(stable)\u001b[39m\n",
      "  x\u001b[36m::Vector{Float64}\u001b[39m\n",
      "Locals\n",
      "  @_3\u001b[33m\u001b[1m::Union{Nothing, Tuple{Float64, Int64}}\u001b[22m\u001b[39m\n",
      "  s\u001b[36m::Float64\u001b[39m\n",
      "  val\u001b[36m::Float64\u001b[39m\n",
      "Body\u001b[36m::Float64\u001b[39m\n",
      "\u001b[90m1 ─\u001b[39m %1  = Main.eltype(x)\u001b[36m::Core.Const(Float64)\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (s = Main.zero(%1))\n",
      "\u001b[90m│  \u001b[39m %3  = x\u001b[36m::Vector{Float64}\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (@_3 = Base.iterate(%3))\n",
      "\u001b[90m│  \u001b[39m %5  = (@_3 === nothing)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m│  \u001b[39m %6  = Base.not_int(%5)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m└──\u001b[39m       goto #4 if not %6\n",
      "\u001b[90m2 ┄\u001b[39m %8  = @_3\u001b[36m::Tuple{Float64, Int64}\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (val = Core.getfield(%8, 1))\n",
      "\u001b[90m│  \u001b[39m %10 = Core.getfield(%8, 2)\u001b[36m::Int64\u001b[39m\n",
      "\u001b[90m│  \u001b[39m       (s = s + val)\n",
      "\u001b[90m│  \u001b[39m       (@_3 = Base.iterate(%3, %10))\n",
      "\u001b[90m│  \u001b[39m %13 = (@_3 === nothing)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m│  \u001b[39m %14 = Base.not_int(%13)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m└──\u001b[39m       goto #4 if not %14\n",
      "\u001b[90m3 ─\u001b[39m       goto #2\n",
      "\u001b[90m4 ┄\u001b[39m       return s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@code_warntype stable(global_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  77.671 μs (3490 allocations: 70.16 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime unstable();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.128 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime stable($global_x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
