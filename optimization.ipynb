{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using StaticArrays\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is just in time (JIT) compiled, so the first time you run code there will be a penalty to first compile the code before it can be run. We can use the `@time` macro to see this effect. (If you run the cell more than once the effect will go away since it has already been compiled!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(1000);\n",
    "function sum_global()\n",
    "    s = 0.0\n",
    "    for i in x\n",
    "        s += i\n",
    "    end\n",
    "    return s\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.010468 seconds (3.68 k allocations: 78.109 KiB, 98.42% compilation time)\n",
      "  0.000129 seconds (3.49 k allocations: 70.156 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time sum_global();\n",
    "@time sum_global();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@time` macro is convenient, but only executes the function once. We can use the `BenchmarkTools.jl` library to investigate the performance of our function with multiple trials. The `@benchmark` macro will run 10,000 trials or 5 seconds whichever comes first. (These can be configured). There is also the `@btime` macro which runs the same number of trials as `@benchmark` but has output similar to `@time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: 10000 samples with 1 evaluation.\n",
       " Range \u001b[90m(\u001b[39m\u001b[36m\u001b[1mmin\u001b[22m\u001b[39m … \u001b[35mmax\u001b[39m\u001b[90m):  \u001b[39m\u001b[36m\u001b[1m74.100 μs\u001b[22m\u001b[39m … \u001b[35m 2.955 ms\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmin … max\u001b[90m): \u001b[39m0.00% … 95.43%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[34m\u001b[1mmedian\u001b[22m\u001b[39m\u001b[90m):     \u001b[39m\u001b[34m\u001b[1m77.100 μs              \u001b[22m\u001b[39m\u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmedian\u001b[90m):    \u001b[39m0.00%\n",
       " Time  \u001b[90m(\u001b[39m\u001b[32m\u001b[1mmean\u001b[22m\u001b[39m ± \u001b[32mσ\u001b[39m\u001b[90m):   \u001b[39m\u001b[32m\u001b[1m79.154 μs\u001b[22m\u001b[39m ± \u001b[32m54.491 μs\u001b[39m  \u001b[90m┊\u001b[39m GC \u001b[90m(\u001b[39mmean ± σ\u001b[90m):  \u001b[39m2.33% ±  3.51%\n",
       "\n",
       "  \u001b[39m \u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▂\u001b[39m▁\u001b[39m \u001b[39m \u001b[39m▁\u001b[39m▆\u001b[34m█\u001b[39m\u001b[39m▄\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[32m \u001b[39m\u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \u001b[39m \n",
       "  \u001b[39m▂\u001b[39m▃\u001b[39m▆\u001b[39m█\u001b[39m█\u001b[39m█\u001b[39m▆\u001b[39m▅\u001b[39m█\u001b[39m█\u001b[34m█\u001b[39m\u001b[39m█\u001b[39m▇\u001b[39m▄\u001b[39m▃\u001b[39m▂\u001b[32m▂\u001b[39m\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▁\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m▂\u001b[39m \u001b[39m▃\n",
       "  74.1 μs\u001b[90m         Histogram: frequency by time\u001b[39m        92.7 μs \u001b[0m\u001b[1m<\u001b[22m\n",
       "\n",
       " Memory estimate\u001b[90m: \u001b[39m\u001b[33m70.16 KiB\u001b[39m, allocs estimate\u001b[90m: \u001b[39m\u001b[33m3490\u001b[39m."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@benchmark sum_global()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function takes parameters, be sure to \"interpolate\" them with a `$` when using `@benchmark` this tells `BenchmarkTools.jl` to ignore the allocation and time required to allocate the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.020 μs (4 allocations: 3.59 KiB)\n",
      "  1.370 μs (5 allocations: 3.95 KiB)\n"
     ]
    }
   ],
   "source": [
    "# No Interpolation\n",
    "@btime inv($(rand(6,6)));\n",
    "# Parameter is Interpolated\n",
    "@btime inv(rand(6,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Memory Allocations\n",
    "One of the easiest ways to speed up your code is by removing all unnecessary memory allocations. In Python the end-user is not able to manage memory easily and the interprer or library (e.g. numpy) is trusted to handle things.\n",
    "\n",
    "Julia can also operate like Python, where we just trust the compiler to \"do the right thing\". This is good for prototyping, but can result in inefficient code. Below we will look at the basics of memory allocations and how to reduce them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------\n",
    "Lets start by allocating a small array of 10,000 Float64s. Each Float64 is 8 bytes (64 bits), so the total size should be 80,000 bytes or 80kB (78.125 kiB). Note there will be a slight overhead for some internal machinery associated with each allocation (e.g. a pointer to the array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000011 seconds (2 allocations: 78.172 KiB)\n"
     ]
    }
   ],
   "source": [
    "@time ones(Float64, 100, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Julia all arrays are `heap` allocated since they are re-sizeable. This just means that the array is further away from the CPU and it can be slow(er) to allocate and access. The first strategy to mitigate this penalty is to simply allocate memory as little as possible. To check this we can use our `BenchmarkTools.jl` library to understand the allocations in our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "allocates_once (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function allocates_alot()\n",
    "    res = 0.0\n",
    "    for i in 1:100\n",
    "        x = rand(100,100) # ALLOCATION EVERY LOOP ITERATION\n",
    "        res += sum(x)\n",
    "    end\n",
    "    return res\n",
    "end\n",
    "\n",
    "function allocates_once()\n",
    "    res = 0.0\n",
    "    storage = zeros(100,100)\n",
    "    for i in 1:100\n",
    "        #The ! point means it mutates the input\n",
    "        # In this case it over writes storage with new random values\n",
    "        rand!(storage)\n",
    "        res += sum(storage)\n",
    "    end\n",
    "    return res\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `@btime` macro we can measure the performace uplift and the number of allocations from each function. You'll find that the function which allocates a new array each loop iteration will be slower and allocate far more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.003 ms (200 allocations: 7.63 MiB)\n",
      "  407.100 μs (2 allocations: 78.17 KiB)\n"
     ]
    }
   ],
   "source": [
    "@btime allocates_alot();\n",
    "@btime allocates_once();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more subtle, yet common place where allocations occur is array slices (e.g. `arr[1:10]`). In Julia these slices allocate a new array by default. We can get around this by telling Julia that we only want to read the data inside the arr. To do this we use an `array view` which can be acheive with the `view(...)` function or the `@views` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "view_macro_slices (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function slices_allocate(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    res = sum(x[1:N])\n",
    "    return res\n",
    "end\n",
    "\n",
    "function view_slices(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    #The view function takes the array\n",
    "    # and the indicies you want a view of.\n",
    "    res = sum(view(x, 1:N))\n",
    "    return res\n",
    "end\n",
    "\n",
    "function view_macro_slices(x)\n",
    "    N = Int(length(x) / 2)\n",
    "    # The @views macro tells Julia all array slices in\n",
    "    # this line should be array views.\n",
    "    @views res = sum(x[1:N])\n",
    "    return res\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see a large speed increase and the number of allocations drop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  37.000 μs (3 allocations: 390.69 KiB)\n",
      "  5.333 μs (1 allocation: 16 bytes)\n",
      "  5.317 μs (1 allocation: 16 bytes)\n"
     ]
    }
   ],
   "source": [
    "x = rand(100000)\n",
    "@btime slices_allocate(x);\n",
    "@btime view_slices(x);\n",
    "@btime view_macro_slices(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StaticArrays\n",
    "Earlier we mentioned that Julia arrays are `heap` allocated because they have variable length (i.e., you can append to them). If you know the length of your array beforehand, you can use the `StaticArrays.jl` library to `stack` allocate your data (if its small enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Stability\n",
    "Julia will do its best to infer the data types of your variables, but if the compiler cannot infer the type of all your variables this leads to type instability and will slow down your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
